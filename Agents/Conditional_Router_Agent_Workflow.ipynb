{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadScientist85/CodingIT/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KIOsbGeHRgm"
      },
      "source": [
        "# Conditional Router Agent Workflow - Selecting the right LLM for the job\n",
        "Author: [Zain Hasan](https://x.com/ZainHasan6)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unQT2mpaHRgr"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we'll demonstrate how to program an agent workflow that dynamically selects LLMs based on their specialties and task requirements.\n",
        "\n",
        "For example, coding tasks might use specialized models like Qwen coder or DeepSeek 2.5, while planning or reasoning tasks could leverage different models optimized for those purposes.\n",
        "\n",
        "To achieve this, we'll create:\n",
        "\n",
        "1. LLM Router: A language model that selects and justifies the best model for a given task\n",
        "2. Simple API that executes the chosen model and solves the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR-2NHMXHRgt"
      },
      "source": [
        "## Conditional Router Agent Workflow\n",
        "\n",
        "<img src=\"https://github.com/togethercomputer/together-cookbook/blob/main/images/if_router.png?raw=1\" width=\"700\">\n",
        "\n",
        "In this **conditional router agent workflow**, we demonstrate how to program a router LLM that selects from multiple routes - including different LLMs, prompts, action sequences, or functionalities. Given a task and route preferences, it chooses one path and justifies its selection. This implementation activates only one path per LLM call.\n",
        "\n",
        "For our specific use case, selecting a model for a task follows these steps:\n",
        "\n",
        "1. Router LLM receives user prompt and route information, outputs JSON with `model_choice` and `reason`\n",
        "2. Simple LLM call executes `model_choice` and returns response\n",
        "\n",
        "Now let's see the coded implementation of this workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBkn9ordHRgw"
      },
      "source": [
        "## Setup and Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2KrUQkyDHRgy"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install -qU pydantic together"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "id": "eDzyD6HiH0Ur",
        "outputId": "077c7da5-9b1c-40d3-dbb6-9643ed4e28dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret secretName does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3196339759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'secretName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret secretName does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a609SL4cHRg1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import together\n",
        "from together import Together\n",
        "\n",
        "from typing import Any, Optional, Dict, List, Literal\n",
        "from pydantic import Field, BaseModel, ValidationError\n",
        "\n",
        "TOGETHER_API_KEY = \"--Your API Key--\"\n",
        "\n",
        "client = Together(api_key= TOGETHER_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q5zOTIHHRg3"
      },
      "outputs": [],
      "source": [
        "# Simple LLM call helper function\n",
        "def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):\n",
        "    \"\"\" Run the language model with the given user prompt and system prompt. \"\"\"\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Simple JSON mode LLM call helper function\n",
        "def JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):\n",
        "    \"\"\" Run a language model with the given user prompt and system prompt, and return a structured JSON object. \"\"\"\n",
        "    try:\n",
        "        messages = []\n",
        "        if system_prompt:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "        extract = client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "            response_format={\n",
        "                \"type\": \"json_object\",\n",
        "                \"schema\": schema.model_json_schema(),\n",
        "            },\n",
        "        )\n",
        "\n",
        "        response = json.loads(extract.choices[0].message.content)\n",
        "        return response\n",
        "\n",
        "    except ValidationError as e:\n",
        "        raise ValueError(f\"Schema validation failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fho0_y7WHRhH"
      },
      "source": [
        "## Routing Agent Implementation\n",
        "\n",
        "The overall flow of what we need to implement will be as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX4jByPrHRhJ"
      },
      "source": [
        "#### Router LLM\n",
        "\n",
        "The router LLM will need to select a route - we will create a Pydantic class to capture all available routes. This class will serve as both a guide for the Router LLM's output format and enable parsing its response to execute the chosen model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCHuwBnWHRhM"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "class ModelOutput(BaseModel):\n",
        "    model: Literal[\"deepseek-ai/DeepSeek-V3\",  # All model choices that can be selected\n",
        "                   \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "                   \"Gryphe/MythoMax-L2-13b\",\n",
        "                   \"Qwen/QwQ-32B-Preview\",\n",
        "                   \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]\n",
        "\n",
        "    reason: str = Field(   # We need the router to tell us why the model/route was selected\n",
        "        description=\"Reason why this model was selected for the task specified in the prompt/query.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDxTqaiHHRhR"
      },
      "outputs": [],
      "source": [
        "# We will use the pydantic class to both structure the output prompt and to give the router LLM information about routes\n",
        "# This will help the router make a better decision on which model to select\n",
        "\n",
        "ROUTER_SYSTEM_PROMPT = \"\"\"Given a user prompt/query, select the best model from the available options to solve the task and provide a reason for your choice.\n",
        "Each model has different capabilities - select the best model for the task provided:\n",
        "- deepseek-ai/DeepSeek-V3: Good generic model to default to incase no better alternative is selected\n",
        "- Qwen/Qwen2.5-Coder-32B-Instruct: Best for code generation tasks\n",
        "- Gryphe/MythoMax-L2-13b: Best model for story-telling and role-play and fantasy tasks\n",
        "- Qwen/QwQ-32B-Preview: Best model for reasoning, math and muiltistep tasks\n",
        "- meta-llama/Llama-3.3-70B-Instruct-Turbo: Best model for general enterprise usecases and tasks\"\"\"\n",
        "\n",
        "ROUTER_PROMPT = \"Given a user prompt/query: {user_query}, select the best model from the available options to solve the task and provide a reason for your choice. Answer only in JSON format.\"\n",
        "\n",
        "prompt = \"Produce python code snippet to check to see if a number is prime or not.\"\n",
        "\n",
        "selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=prompt),\n",
        "                            ModelOutput,\n",
        "                            system_prompt=ROUTER_SYSTEM_PROMPT)\n",
        "\n",
        "selected_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlH1doAhHRhZ"
      },
      "source": [
        "Lets create a small function that we can execute for multiple tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myB0ERttHRhb"
      },
      "outputs": [],
      "source": [
        "# Write a function that will call the router and then the output llm model in sequence to generate a response to a user prompt.\n",
        "def run_router_workflow(user_prompt : str):\n",
        "\n",
        "    # Which route to take\n",
        "    selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=user_prompt),\n",
        "                            ModelOutput,\n",
        "                            system_prompt=ROUTER_SYSTEM_PROMPT)\n",
        "\n",
        "    # Take that route and run the LLM model\n",
        "    response = run_llm(user_prompt= user_prompt,\n",
        "                   model = selected_model['model']\n",
        "    )\n",
        "    return selected_model['model'], selected_model['reason'], response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zVQcC1PHRhd"
      },
      "outputs": [],
      "source": [
        "model, reason, response = run_router_workflow(prompt)\n",
        "\n",
        "print(f\"Query: {prompt}\")\n",
        "print(20*'==')\n",
        "print(f\"Selected Model: {model} \\n Reason: {reason}\")\n",
        "print(20*'==')\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRnxptdHRhg"
      },
      "source": [
        "### Generic Implementation\n",
        "\n",
        "Now that we know how the internals of this workflow execute we can write a more generic version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ-XyacfHRhk"
      },
      "outputs": [],
      "source": [
        "def router_workflow(input_query: str, routes : Dict[str, str]) -> str:\n",
        "    \"\"\" Given a `input_qeury` and a dictionary of `routes` containing options and details for each.\n",
        "    Selects the best model for the task and return the response from the model.\n",
        "    \"\"\"\n",
        "    ROUTER_PROMPT = \"\"\"Given a user prompt/query: {user_query}, select the best option out of the following routes:\n",
        "    {routes}. Answer only in JSON format.\"\"\"\n",
        "\n",
        "    # Create a schema from the routes dictionary\n",
        "    class Schema(BaseModel):\n",
        "        route: Literal[tuple(routes.keys())]\n",
        "\n",
        "        reason: str = Field(\n",
        "            description=\"Short one-liner explanation why this route was selected for the task in the prompt/query.\"\n",
        "        )\n",
        "\n",
        "    # Call LLM to select route\n",
        "    selected_route = JSON_llm(ROUTER_PROMPT.format(user_query=input_query, routes=routes), Schema)\n",
        "    print(f\"Selected route:{selected_route['route']}\\nReason: {selected_route['reason']}\\n\")\n",
        "\n",
        "    # Use LLM on selected route.\n",
        "    # Could also have different prompts that need to be used for each route.\n",
        "    response = run_llm(user_prompt= input_query, model = selected_route['route'])\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5zei6UhHRhn"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "prompt_list = [\"Produce python snippet to check to see if a number is prime or not.\",\n",
        "               \"Plan and provide a short itenary for a 2 week vacation in Europe.\",\n",
        "               \"Write a short story about a dragon and a knight.\"]\n",
        "\n",
        "model_routes = { # feel free to add more models and their descriptions\n",
        "    \"Qwen/Qwen2.5-Coder-32B-Instruct\" : \"Best model choice for code generation tasks.\",\n",
        "    \"Gryphe/MythoMax-L2-13b\" : \"Best model choice for story-telling, role-playing and fantasy tasks.\",\n",
        "    \"Qwen/QwQ-32B-Preview\" : \"Best model for reasoning, planning and muilti-step tasks\",\n",
        "}\n",
        "\n",
        "for i, prompt in enumerate(prompt_list):\n",
        "    print(f\"Task {i+1}: {prompt}\\n\")\n",
        "    print(20*'==')\n",
        "    router_workflow(prompt, model_routes)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}